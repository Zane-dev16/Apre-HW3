\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{minted}
\usepackage{enumitem}

\usepackage{indentfirst}

\usepackage[export]{adjustbox} % Align images

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\newtcbtheorem[auto counter,number within=section]{code}{Código}{
  colback=LightOrange!20,
  colframe=LightOrange,
  colbacktitle=LightOrange,
  fonttitle=\bfseries\color{black},
  boxed title style={size=small,colframe=LightOrange},
}{code}

\setstretch{1.2}
\geometry{
  textheight=22.5cm,
  textwidth=13.75cm,
  top=2.5cm,
  headheight=12pt,
  headsep=25pt,
  footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
% ------------------------------------------------------------------------------
\begin{center}
  \begin{figure}
    \includegraphics[scale = 0.3, left]{img/IST_A.eps} % IST logo
    \end{figure}
  \LARGE{ \normalsize \textsc{} \\
  [2.0cm] 
  \LARGE{ \LARGE \textsc{Aprendizagem}} \\
  [1cm]
  \LARGE{ \LARGE \textsc{LEIC IST-UL}} \\
  [1cm]
  \HRule{1.5pt} \\
  [0.4cm]
  \LARGE \textbf{\uppercase{Relatório - Homework 3}}
  \HRule{1.5pt}
  \\ [2.5cm]
  }
\end{center}

\begin{flushleft}
  \textbf{\LARGE Grupo 10:}
\end{flushleft}

\begin{center}
  \begin{minipage}{0.7\textwidth}
      \begin{flushleft}
        \large Gabriel Ferreira \\
        \large  Irell Zane
      \end{flushleft}
  \end{minipage}%
  \begin{minipage}{0.3\textwidth}
      \begin{flushright}
        \large 107030\\
        \large 107161
      \end{flushright}
  \end{minipage}
\end{center}

\begin{center}
  \vspace{4cm}
  \date \large \bf  2024/2025 -- 1st Semester, P1
\end{center}

\setcounter{page}{0}
\thispagestyle{empty}
\renewcommand{\thesection}{\Roman{section}}

\newpage

% ------------------------------------------------------------------------------
% Content
% ------------------------------------------------------------------------------



\large{\textbf{Part I}: Pen and paper}\normalsize

\begin{enumerate}[leftmargin=\labelsep]
\item 

Given the polynomial basis function:

\[\phi(y_1, y_2) = y_1 \times y_2\]

We apply this to our input data:

\begin{align*}
x_1: \phi(1, 1) &= 1 \times 1 = 1 \\
x_2: \phi(1, 3) &= 1 \times 3 = 3 \\
x_3: \phi(3, 2) &= 3 \times 2 = 6 \\
x_4: \phi(3, 3) &= 3 \times 3 = 9 \\
x_5: \phi(2, 4) &= 2 \times 4 = 8
\end{align*}

For OLS, we need $\mathbf{X}$ (input) and $\mathbf{y}$ (output) matrices:

\[\mathbf{X} = \begin{bmatrix}
1 & 1 \\
1 & 3 \\
1 & 6 \\
1 & 9 \\
1 & 8
\end{bmatrix}\]

\[\mathbf{z} = \begin{bmatrix}
1.25 \\
7.0 \\
2.7 \\
3.2 \\
5.5
\end{bmatrix}\]

\textbf{OLS closed form solution calculation}

The OLS closed form solution is given by:

\[\boldsymbol{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{z}\]

Calculation of the separate components of formula:

\[
\mathbf{X}^T \mathbf{X} = 
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 \\
1 & 3 & 6 & 9 & 8
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
1 & 3 \\
1 & 6 \\
1 & 9 \\
1 & 8
\end{bmatrix}
=
\begin{bmatrix}
5 & 27 \\
27 & 191
\end{bmatrix}
\]

\[(\mathbf{X}^T \mathbf{X})^{-1} \approx \begin{bmatrix}
0.845 & -0.119 \\
-0.119 & 0.022
\end{bmatrix}\]

\[\mathbf{X}^T \mathbf{z} = \begin{bmatrix}
19.65 \\
111.25
\end{bmatrix}\]

Finally,

\[\boldsymbol{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{z}\]
\[= \begin{bmatrix}
0.845 & -0.119 \\
-0.119 & 0.022
\end{bmatrix} \times \begin{bmatrix}
19.65 \\
111.25
\end{bmatrix}\]

\[\approx \begin{bmatrix}
3.316\\
0.114
\end{bmatrix}\]

Therefore, the regression model in the transformed space is:

\[y_{num} = 3.316 + 0.114 \times \phi(y_1, y_2)\]

\item 
\textbf{Ridge regression closed form solution calculation}\\
The ridge regression closed form solution with penalty factor $\lambda$ = 1 is given by:
\[\boldsymbol{w} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{z}\]
Calculation of the separate components of formula:
\[
\mathbf{X}^T \mathbf{X} = 
\begin{bmatrix}
1 & 1 & 1 & 1 & 1 \\
1 & 3 & 6 & 9 & 8
\end{bmatrix}
\begin{bmatrix}
1 & 1 \\
1 & 3 \\
1 & 6 \\
1 & 9 \\
1 & 8
\end{bmatrix}
=
\begin{bmatrix}
5 & 27 \\
27 & 191
\end{bmatrix}
\]
\[\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} = \begin{bmatrix}
5 & 27 \\
27 & 191
\end{bmatrix} + \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} = \begin{bmatrix}
6 & 27 \\
27 & 192
\end{bmatrix}\]
\[(\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \approx \begin{bmatrix}
0.454 & -0.064 \\
-0.064 & 0.014
\end{bmatrix}\]
\[\mathbf{X}^T \mathbf{z} = \begin{bmatrix}
19.65 \\
111.25
\end{bmatrix}\]
Finally,
\[\boldsymbol{w} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{z}\]
\[= \begin{bmatrix}
0.454 & -0.064 \\
-0.064 & 0.014
\end{bmatrix} \times \begin{bmatrix}
19.65 \\
111.25
\end{bmatrix}\]
\[\approx \begin{bmatrix}
1.818\\
0.324
\end{bmatrix}\]
Therefore, the ridge regression model in the transformed space with $\sigma$ = 1 is:
\[y_{num} = 1.818 + 0.324 \times \phi(y_1, y_2)\]

The impact of ridge regression on the coefficients can be observed by comparing the Ordinary Least Squares (OLS) solution with the ridge regression solution (using $\lambda = 1$):

\begin{align*}
\text{OLS coefficients:} & \quad \boldsymbol{w}_{\text{OLS}} \approx \begin{bmatrix} 3.316 \\ 0.114 \end{bmatrix} \\[10pt]
\text{Ridge coefficients:} & \quad \boldsymbol{w}_{\text{Ridge}} \approx \begin{bmatrix} 1.818 \\ 0.324 \end{bmatrix}
\end{align*}

The effects of ridge regression are as follows:
The larger coefficient (intercept) has been substantially reduced from 3.316 to 1.818.
The smaller coefficient increased from.
This aligns with the primary goal of ridge regression to shrink large coefficients, which are more heavily penalized due to the quadratic regularization term.

Ultimately the Ridge regression has reduced the sum of squared coefficients:
\begin{align*}
\text{OLS:} & \quad 3.316^2 + 0.114^2 \approx 11.00 \\
\text{Ridge:} & \quad 1.818^2 + 0.324^2 \approx 3.41
\end{align*}
This significant reduction in the sum of squared coefficients indicates a less complex model, which is likely to generalize better to unseen data.

\item 
    \textbf{OLS and Ridge prediction calculations}\\

For $x_6 = (2, 2, 0.7)$:
\begin{align*}
\phi(2, 2) &= 2 \times 2 = 4 \\
y_{OLS} &= 3.316 + 0.114 \times 4 = 3.772 \\
y_{Ridge} &= 1.818 + 0.324 \times 4 = 3.114
\end{align*}

For $x_7 = (1, 2, 1.1)$:
\begin{align*}
\phi(1, 2) &= 1 \times 2 = 2 \\
y_{OLS} &= 3.316 + 0.114 \times 2 = 3.544 \\
y_{Ridge} &= 1.818 + 0.324 \times 2 = 2.466
\end{align*}

For $x_8 = (5, 1, 2.2)$:
\begin{align*}
\phi(5, 1) &= 5 \times 1 = 5 \\
y_{OLS} &= 3.316 + 0.114 \times 5 = 3.886 \\
y_{Ridge} &= 1.818 + 0.324 \times 5 = 3.438
\end{align*}

\begin{table}[h]
\centering
\begin{tabular}{cccccc}
\hline
Observation & $y_1$ & $y_2$ & $\phi(y_1, y_2)$ & $y_{OLS}$ & $y_{Ridge}$ \\
\hline
$x_6$ & 2 & 2 & 4 & 3.772 & 3.114 \\
$x_7$ & 1 & 2 & 2 & 3.544 & 2.466 \\
$x_8$ & 5 & 1 & 5 & 3.886 & 3.438 \\
\hline
\end{tabular}
\caption{Predicted values for test observations}
\end{table}

\textbf{RMSE calculation of both models}

\begin{align*}
RMSE_{OLS} &= \sqrt{\frac{1}{3}\sum_{i=6}^8 (y_i - \hat{y}_i)^2} \\
&= \sqrt{\frac{1}{3}[(0.7 - 3.772)^2 + (1.1 - 3.544)^2 + (2.2 - 3.886)^2]} \\
&\approx 2.467
\end{align*}

\begin{align*}
RMSE_{Ridge} &= \sqrt{\frac{1}{3}\sum_{i=6}^8 (y_i - \hat{y}_i)^2} \\
&= \sqrt{\frac{1}{3}[(0.7 - 3.114)^2 + (1.1 - 2.466)^2 + (2.2 - 3.438)^2]} \\
&\approx 1.748
\end{align*}

The results show that the Ridge regression model has a lower RMSE (1.748) compared to the OLS model (2.467) for the given test data. This aligns with our expectations as the objective of the regularization in ridge regression was so that the model overfits the training data less and generalize better for unseen data.

\item MLP Propagation:

\begin{equation*}
  x^{[1]} = z^{[1]} = W[1] x^{[0]} + b^{[1]} =
  \begin{pmatrix}
    0.1 & 0.1 \\
    0.1 & 0.2 \\
    0.2 & 0.1 
  \end{pmatrix}
  \begin{pmatrix}
    1 & 1 
  \end{pmatrix} +
  \begin{pmatrix}
    0.1 \\
    0 \\
    0.1
  \end{pmatrix} =
  \begin{pmatrix}
    0.3 \\
    0.3 \\
    0.4
  \end{pmatrix}
\end{equation*}
\begin{equation*}
  z^{[2]} = W[2] x^{[1]} + b^{[2]} =
  \begin{pmatrix}
    1 & 2 & 2 \\
    1 & 2 & 1 \\
    1 & 1 & 1
  \end{pmatrix}
  \begin{pmatrix}
    0.3 \\
    0.3 \\
    0.4
  \end{pmatrix} +
  \begin{pmatrix}
    1 \\
    1 \\
    1
  \end{pmatrix} =
  \begin{pmatrix}
    2.7 \\
    2.3 \\
    2
  \end{pmatrix}
\end{equation*}

\begin{equation*}
  x^{[2]} = softmax(z^{[2]}) =
  \begin{pmatrix}
    0.46 \\
    0.31 \\
    0.23
  \end{pmatrix}
\end{equation*}

Backpropagation:
\begin{equation*}
  \delta^{[2]}=\frac{\partial E}{\partial x^{[2]}} \circ \frac{\partial x^{[2]}}{\partial z^{[2]}} = x^{[2]} - t = 
  \begin{pmatrix}
    0.46 - 0 \\
    0.31 - 1 \\
    0.23 - 0
    \end{pmatrix} =
    \begin{pmatrix}
      0.46 \\
      -0.69 \\
      0.23
      \end{pmatrix}
\end{equation*}

Layer 2 weights:

\begin{equation*}
  \frac{\partial E}{\partial W^{[2]}} = \delta^{[2]} (x^{[1]})^T =
  \begin{pmatrix}
    0.46 \\
    -0.69 \\
    0.23
    \end{pmatrix}
    \begin{pmatrix}
      0.3 & 0.3 & 0.4
      \end{pmatrix} =
      \begin{pmatrix}
        0,14  & 0,14  & 0,18  \\
        -0,21 & -0,21 & -0,28 \\
        0,07  & 0,07  & 0,09
      \end{pmatrix}
\end{equation*}

\begin{equation*}
  W^{[2]}_{new} = W^{[2]}_{old} - \eta \frac{\partial E}{\partial W^{[2]}} =
  \begin{pmatrix}
    1 & 2 & 2 \\
    1 & 2 & 1 \\
    1 & 1 & 1
  \end{pmatrix} - 0.1
  \begin{pmatrix}
    0.81 & 0.81 & 1.08 \\
    0.39 & 0.39 & 0.52 \\
    0.6	 & 0.6 &  0.8
    \end{pmatrix} =
    \begin{pmatrix}
      0.99 & 1.99 & 1.98 \\
      1.02 & 2.02 & 1.03 \\
      0.99 & 0.99 & 0.99
      \end{pmatrix}
\end{equation*}


Layer 2 biases:

\begin{equation*}
  \frac{\partial E}{\partial b^{[2]}} =
  \delta^{[2]} \frac{\partial z^{[2]^T}}{\partial b^{[2]}} =
  \delta^{[2]} =
  \begin{pmatrix}
    0.46 \\
    -0.69 \\
    0.23
    \end{pmatrix}
\end{equation*}

\begin{equation*}
  b^{[2]}_{new} =
  b^{[2]}_{old} - \eta \frac{\partial E}{\partial b^{[2]}} =
  \begin{pmatrix}
    1 \\
    1 \\
    1
    \end{pmatrix} - 0.1
    \begin{pmatrix}
      0.46 \\
      -0.69 \\
      0.23
      \end{pmatrix} =
    \begin{pmatrix}
      0.95 \\
      1.07 \\
      0.98
      \end{pmatrix}
\end{equation*}

Layer 1 weights:

\begin{equation*}
  \delta^{[1]}=\left( \frac{\partial z^{[2]}}{\partial x^{[1]}}\right)^T \cdot \delta^{[2]} \circ \frac{\partial x^{[1]}}{\partial z^{[1]}}=
  (W^{[2]})^T\delta^{[2]} \times 1 =
  \begin{pmatrix}
    0.00  \\
    -0.23 \\
    0.46
    \end{pmatrix}
\end{equation*}

\begin{equation*}
  \frac{\partial E}{\partial W^{[1]}} = \delta^{[1]} (x^{[0]})^T =
  \begin{pmatrix}
    0.00  \\
    -0.23 \\
    0.46
    \end{pmatrix}
  \begin{pmatrix}
    1 & 1
  \end{pmatrix} =
  \begin{pmatrix}
    0.00  & 0.00 \\
    -0.23 & -0.23 \\
    0.46  & 0.46
  \end{pmatrix}
\end{equation*}

\begin{equation*}
  W^{[1]}_{new} = W^{[1]}_{old} - \eta \frac{\partial E}{\partial W^{[1]}} =
  \begin{pmatrix}
    0.1 & 0.1 \\
    0.1 & 0.2 \\
    0.2 & 0.1 
  \end{pmatrix} - 0.1
  \begin{pmatrix}
    0.00  & 0.00 \\
    -0.23 & -0.23 \\
    0.46  & 0.46
  \end{pmatrix} =
  \begin{pmatrix}
    0.10 & 0.10 \\
    0.12 & 0.22 \\
    0.15 & 0.05 
  \end{pmatrix} 
\end{equation*}

Layer 1 biases:

\begin{equation*}
  \frac{\partial E}{\partial b^{[1]}} =
  \delta^{[1]} \frac{\partial z^{[1]^T}}{\partial b^{[1]}} =
  \delta^{[1]} =
  \begin{pmatrix}
    0.00  \\
    -0.23 \\
    0.46
    \end{pmatrix}
\end{equation*}

\begin{equation*}
  b^{[1]}_{new} =
  b^{[1]}_{old} - \eta \frac{\partial E}{\partial b^{[1]}} =
  \begin{pmatrix}
    0.1 \\
    0 \\
    0.1
  \end{pmatrix} - 0.1
  \begin{pmatrix}
    0.00  \\
    -0.23 \\
    0.46
    \end{pmatrix} =
  \begin{pmatrix}
    0.10 \\
    0.02 \\
    0.05
    \end{pmatrix}
\end{equation*}

\end{enumerate}

\large{\textbf{Part II}: Programming}\normalsize

\begin{enumerate}[leftmargin=\labelsep,resume]
\item Solution to the programming questions here.
\end{enumerate}

\vskip 1cm
\textbf{End note}: do not forget to also submit your Jupyter notebook

\newpage

% ----------------------------------------------------------------------
% Cover
% ----------------------------------------------------------------------

\end{document}

